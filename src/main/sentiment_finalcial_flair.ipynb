{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from flair.datasets import UD_ENGLISH\n",
    "from flair.embeddings import WordEmbeddings, StackedEmbeddings\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../../Data/Training/finalcial_training_dataset_4837.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('../../Data/Training/finacial_data_train_4897/finalcial_training_dataset_4837.csv', encoding=\"ISO-8859-1\" )\n",
    "\n",
    "df_train, df_val = train_test_split(df, test_size=0.2, shuffle = True)\n",
    "df_train.to_csv('../../Data/Training/finacial_data_train_4897/train.csv')\n",
    "df_val.to_csv('../../Data/Training/finacial_data_train_4897/val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Yuji\\Desktop\\yxf965\\src\\main\n",
      "2022-04-06 10:12:33,427 Reading data from ..\\..\\Data\\Training\\finacial_data_train_4897\n",
      "2022-04-06 10:12:33,427 Train: ..\\..\\Data\\Training\\finacial_data_train_4897\\train.csv\n",
      "2022-04-06 10:12:33,428 Dev: ..\\..\\Data\\Training\\finacial_data_train_4897\\val.csv\n",
      "2022-04-06 10:12:33,428 Test: ..\\..\\Data\\Training\\finacial_data_train_4897\\test.csv\n",
      "Corpus: 3877 train + 970 dev + 214 test sentences\n",
      "2022-04-06 10:12:33,451 Computing label dictionary. Progress:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3877/3877 [00:01<00:00, 2952.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 10:12:34,768 Corpus contains the labels: polarity (#3877)\n",
      "2022-04-06 10:12:34,768 Created (for label 'polarity') Dictionary with 4 tags: <unk>, neutral, negative, positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 10:12:39,093 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,094 Model: \"TextClassifier(\n",
      "  (loss_function): CrossEntropyLoss()\n",
      "  (document_embeddings): TransformerDocumentEmbeddings(\n",
      "    (model): DistilBertModel(\n",
      "      (embeddings): Embeddings(\n",
      "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "        (position_embeddings): Embedding(512, 768)\n",
      "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (transformer): Transformer(\n",
      "        (layer): ModuleList(\n",
      "          (0): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (1): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (2): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (3): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (4): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "          (5): TransformerBlock(\n",
      "            (attention): MultiHeadSelfAttention(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
      "            )\n",
      "            (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (ffn): FFN(\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "              (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            )\n",
      "            (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Linear(in_features=768, out_features=4, bias=True)\n",
      "  (weights): None\n",
      "  (weight_tensor) None\n",
      ")\"\n",
      "2022-04-06 10:12:39,095 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,095 Corpus: \"Corpus: 3877 train + 970 dev + 214 test sentences\"\n",
      "2022-04-06 10:12:39,096 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,096 Parameters:\n",
      "2022-04-06 10:12:39,096  - learning_rate: \"5e-05\"\n",
      "2022-04-06 10:12:39,097  - mini_batch_size: \"4\"\n",
      "2022-04-06 10:12:39,097  - patience: \"3\"\n",
      "2022-04-06 10:12:39,097  - anneal_factor: \"0.5\"\n",
      "2022-04-06 10:12:39,098  - max_epochs: \"10\"\n",
      "2022-04-06 10:12:39,098  - shuffle: \"True\"\n",
      "2022-04-06 10:12:39,099  - train_with_dev: \"False\"\n",
      "2022-04-06 10:12:39,099  - batch_growth_annealing: \"False\"\n",
      "2022-04-06 10:12:39,099 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,100 Model training base path: \"resources\\taggers\\financial_models\"\n",
      "2022-04-06 10:12:39,100 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,100 Device: cpu\n",
      "2022-04-06 10:12:39,101 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:12:39,101 Embeddings storage mode: none\n",
      "2022-04-06 10:12:39,105 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:14:25,435 epoch 1 - iter 97/970 - loss 0.30834835 - samples/sec: 5.43 - lr: 0.000005\n",
      "2022-04-06 10:15:36,210 epoch 1 - iter 194/970 - loss 0.26919730 - samples/sec: 5.49 - lr: 0.000010\n",
      "2022-04-06 10:16:55,000 epoch 1 - iter 291/970 - loss 0.23718234 - samples/sec: 4.93 - lr: 0.000015\n",
      "2022-04-06 10:18:10,476 epoch 1 - iter 388/970 - loss 0.21668459 - samples/sec: 5.14 - lr: 0.000020\n",
      "2022-04-06 10:19:26,985 epoch 1 - iter 485/970 - loss 0.20546639 - samples/sec: 5.07 - lr: 0.000025\n",
      "2022-04-06 10:20:43,307 epoch 1 - iter 582/970 - loss 0.19246738 - samples/sec: 5.09 - lr: 0.000030\n",
      "2022-04-06 10:22:02,905 epoch 1 - iter 679/970 - loss 0.18411665 - samples/sec: 4.88 - lr: 0.000035\n",
      "2022-04-06 10:23:16,493 epoch 1 - iter 776/970 - loss 0.17697358 - samples/sec: 5.28 - lr: 0.000040\n",
      "2022-04-06 10:24:22,804 epoch 1 - iter 873/970 - loss 0.17348427 - samples/sec: 5.85 - lr: 0.000045\n",
      "2022-04-06 10:25:32,596 epoch 1 - iter 970/970 - loss 0.16780761 - samples/sec: 5.56 - lr: 0.000050\n",
      "2022-04-06 10:25:33,329 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:25:33,330 EPOCH 1 done: loss 0.1678 - lr 0.0000500\n",
      "2022-04-06 10:26:24,530 DEV : loss 0.11725503951311111 - f1-score (micro avg)  0.8443\n",
      "2022-04-06 10:26:24,781 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 10:26:24,783 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:28:06,360 epoch 2 - iter 97/970 - loss 0.10032835 - samples/sec: 5.73 - lr: 0.000049\n",
      "2022-04-06 10:29:14,404 epoch 2 - iter 194/970 - loss 0.11241359 - samples/sec: 5.71 - lr: 0.000049\n",
      "2022-04-06 10:30:23,039 epoch 2 - iter 291/970 - loss 0.10736655 - samples/sec: 5.66 - lr: 0.000048\n",
      "2022-04-06 10:31:31,691 epoch 2 - iter 388/970 - loss 0.10790288 - samples/sec: 5.66 - lr: 0.000048\n",
      "2022-04-06 10:32:38,986 epoch 2 - iter 485/970 - loss 0.10236751 - samples/sec: 5.77 - lr: 0.000047\n",
      "2022-04-06 10:33:46,799 epoch 2 - iter 582/970 - loss 0.09801170 - samples/sec: 5.73 - lr: 0.000047\n",
      "2022-04-06 10:34:53,354 epoch 2 - iter 679/970 - loss 0.09821110 - samples/sec: 5.83 - lr: 0.000046\n",
      "2022-04-06 10:36:00,655 epoch 2 - iter 776/970 - loss 0.09523923 - samples/sec: 5.78 - lr: 0.000046\n",
      "2022-04-06 10:37:10,195 epoch 2 - iter 873/970 - loss 0.09583843 - samples/sec: 5.58 - lr: 0.000045\n",
      "2022-04-06 10:38:21,881 epoch 2 - iter 970/970 - loss 0.09549412 - samples/sec: 5.42 - lr: 0.000044\n",
      "2022-04-06 10:38:22,629 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:38:22,629 EPOCH 2 done: loss 0.0955 - lr 0.0000444\n",
      "2022-04-06 10:39:22,138 DEV : loss 0.13390499353408813 - f1-score (micro avg)  0.8423\n",
      "2022-04-06 10:39:22,377 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 10:39:22,379 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:41:07,154 epoch 3 - iter 97/970 - loss 0.04539904 - samples/sec: 5.78 - lr: 0.000044\n",
      "2022-04-06 10:42:18,041 epoch 3 - iter 194/970 - loss 0.04135576 - samples/sec: 5.48 - lr: 0.000043\n",
      "2022-04-06 10:43:28,338 epoch 3 - iter 291/970 - loss 0.04548952 - samples/sec: 5.52 - lr: 0.000043\n",
      "2022-04-06 10:44:43,262 epoch 3 - iter 388/970 - loss 0.04702535 - samples/sec: 5.18 - lr: 0.000042\n",
      "2022-04-06 10:45:53,629 epoch 3 - iter 485/970 - loss 0.04576559 - samples/sec: 5.52 - lr: 0.000042\n",
      "2022-04-06 10:47:11,727 epoch 3 - iter 582/970 - loss 0.04387524 - samples/sec: 4.97 - lr: 0.000041\n",
      "2022-04-06 10:48:21,249 epoch 3 - iter 679/970 - loss 0.04514083 - samples/sec: 5.58 - lr: 0.000041\n",
      "2022-04-06 10:49:35,253 epoch 3 - iter 776/970 - loss 0.04409587 - samples/sec: 5.25 - lr: 0.000040\n",
      "2022-04-06 10:50:48,392 epoch 3 - iter 873/970 - loss 0.04644269 - samples/sec: 5.31 - lr: 0.000039\n",
      "2022-04-06 10:51:51,379 epoch 3 - iter 970/970 - loss 0.04480609 - samples/sec: 6.16 - lr: 0.000039\n",
      "2022-04-06 10:51:52,060 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:51:52,060 EPOCH 3 done: loss 0.0448 - lr 0.0000389\n",
      "2022-04-06 10:52:42,061 DEV : loss 0.17244261503219604 - f1-score (micro avg)  0.8454\n",
      "2022-04-06 10:52:42,290 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 10:52:42,292 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 10:54:18,064 epoch 4 - iter 97/970 - loss 0.01358382 - samples/sec: 6.21 - lr: 0.000038\n",
      "2022-04-06 10:55:21,529 epoch 4 - iter 194/970 - loss 0.01415073 - samples/sec: 6.12 - lr: 0.000038\n",
      "2022-04-06 10:56:24,537 epoch 4 - iter 291/970 - loss 0.01606192 - samples/sec: 6.16 - lr: 0.000037\n",
      "2022-04-06 10:57:27,447 epoch 4 - iter 388/970 - loss 0.01971518 - samples/sec: 6.17 - lr: 0.000037\n",
      "2022-04-06 10:58:30,612 epoch 4 - iter 485/970 - loss 0.02023126 - samples/sec: 6.15 - lr: 0.000036\n",
      "2022-04-06 10:59:32,651 epoch 4 - iter 582/970 - loss 0.01752207 - samples/sec: 6.26 - lr: 0.000036\n",
      "2022-04-06 11:00:36,829 epoch 4 - iter 679/970 - loss 0.02094272 - samples/sec: 6.05 - lr: 0.000035\n",
      "2022-04-06 11:01:38,792 epoch 4 - iter 776/970 - loss 0.01998005 - samples/sec: 6.27 - lr: 0.000034\n",
      "2022-04-06 11:02:41,228 epoch 4 - iter 873/970 - loss 0.01936434 - samples/sec: 6.22 - lr: 0.000034\n",
      "2022-04-06 11:03:43,911 epoch 4 - iter 970/970 - loss 0.02106460 - samples/sec: 6.19 - lr: 0.000033\n",
      "2022-04-06 11:03:44,640 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:03:44,641 EPOCH 4 done: loss 0.0211 - lr 0.0000333\n",
      "2022-04-06 11:04:34,938 DEV : loss 0.19532044231891632 - f1-score (micro avg)  0.8464\n",
      "2022-04-06 11:04:35,174 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 11:04:35,175 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:06:11,038 epoch 5 - iter 97/970 - loss 0.01044759 - samples/sec: 6.20 - lr: 0.000033\n",
      "2022-04-06 11:07:13,617 epoch 5 - iter 194/970 - loss 0.01095818 - samples/sec: 6.20 - lr: 0.000032\n",
      "2022-04-06 11:08:15,500 epoch 5 - iter 291/970 - loss 0.00813395 - samples/sec: 6.27 - lr: 0.000032\n",
      "2022-04-06 11:09:17,683 epoch 5 - iter 388/970 - loss 0.00795027 - samples/sec: 6.24 - lr: 0.000031\n",
      "2022-04-06 11:10:20,568 epoch 5 - iter 485/970 - loss 0.00968502 - samples/sec: 6.17 - lr: 0.000031\n",
      "2022-04-06 11:11:23,411 epoch 5 - iter 582/970 - loss 0.00828464 - samples/sec: 6.18 - lr: 0.000030\n",
      "2022-04-06 11:12:26,320 epoch 5 - iter 679/970 - loss 0.01018186 - samples/sec: 6.18 - lr: 0.000029\n",
      "2022-04-06 11:13:28,047 epoch 5 - iter 776/970 - loss 0.01053092 - samples/sec: 6.29 - lr: 0.000029\n",
      "2022-04-06 11:14:30,807 epoch 5 - iter 873/970 - loss 0.01028374 - samples/sec: 6.19 - lr: 0.000028\n",
      "2022-04-06 11:15:33,698 epoch 5 - iter 970/970 - loss 0.00976995 - samples/sec: 6.17 - lr: 0.000028\n",
      "2022-04-06 11:15:34,392 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:15:34,393 EPOCH 5 done: loss 0.0098 - lr 0.0000278\n",
      "2022-04-06 11:16:25,494 DEV : loss 0.22811159491539001 - f1-score (micro avg)  0.8598\n",
      "2022-04-06 11:16:25,734 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 11:16:25,735 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:18:02,417 epoch 6 - iter 97/970 - loss 0.01309889 - samples/sec: 6.12 - lr: 0.000027\n",
      "2022-04-06 11:19:04,875 epoch 6 - iter 194/970 - loss 0.01058027 - samples/sec: 6.22 - lr: 0.000027\n",
      "2022-04-06 11:20:07,052 epoch 6 - iter 291/970 - loss 0.00855864 - samples/sec: 6.24 - lr: 0.000026\n",
      "2022-04-06 11:21:10,010 epoch 6 - iter 388/970 - loss 0.00649713 - samples/sec: 6.17 - lr: 0.000026\n",
      "2022-04-06 11:22:13,030 epoch 6 - iter 485/970 - loss 0.00741493 - samples/sec: 6.16 - lr: 0.000025\n",
      "2022-04-06 11:23:15,348 epoch 6 - iter 582/970 - loss 0.00788385 - samples/sec: 6.23 - lr: 0.000024\n",
      "2022-04-06 11:24:17,844 epoch 6 - iter 679/970 - loss 0.00787253 - samples/sec: 6.21 - lr: 0.000024\n",
      "2022-04-06 11:25:19,770 epoch 6 - iter 776/970 - loss 0.00734548 - samples/sec: 6.27 - lr: 0.000023\n",
      "2022-04-06 11:26:22,647 epoch 6 - iter 873/970 - loss 0.00843673 - samples/sec: 6.17 - lr: 0.000023\n",
      "2022-04-06 11:27:24,700 epoch 6 - iter 970/970 - loss 0.00830267 - samples/sec: 6.26 - lr: 0.000022\n",
      "2022-04-06 11:27:25,368 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:27:25,368 EPOCH 6 done: loss 0.0083 - lr 0.0000222\n",
      "2022-04-06 11:28:15,857 DEV : loss 0.22754523158073425 - f1-score (micro avg)  0.8515\n",
      "2022-04-06 11:28:16,076 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 11:28:16,078 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:29:53,197 epoch 7 - iter 97/970 - loss 0.00224650 - samples/sec: 6.09 - lr: 0.000022\n",
      "2022-04-06 11:30:55,580 epoch 7 - iter 194/970 - loss 0.00479549 - samples/sec: 6.22 - lr: 0.000021\n",
      "2022-04-06 11:31:58,502 epoch 7 - iter 291/970 - loss 0.00344941 - samples/sec: 6.17 - lr: 0.000021\n",
      "2022-04-06 11:33:01,434 epoch 7 - iter 388/970 - loss 0.00313621 - samples/sec: 6.17 - lr: 0.000020\n",
      "2022-04-06 11:34:03,667 epoch 7 - iter 485/970 - loss 0.00488522 - samples/sec: 6.24 - lr: 0.000019\n",
      "2022-04-06 11:35:06,269 epoch 7 - iter 582/970 - loss 0.00497431 - samples/sec: 6.20 - lr: 0.000019\n",
      "2022-04-06 11:36:09,171 epoch 7 - iter 679/970 - loss 0.00498735 - samples/sec: 6.17 - lr: 0.000018\n",
      "2022-04-06 11:37:12,608 epoch 7 - iter 776/970 - loss 0.00437690 - samples/sec: 6.12 - lr: 0.000018\n",
      "2022-04-06 11:38:14,784 epoch 7 - iter 873/970 - loss 0.00401449 - samples/sec: 6.24 - lr: 0.000017\n",
      "2022-04-06 11:39:16,532 epoch 7 - iter 970/970 - loss 0.00463139 - samples/sec: 6.29 - lr: 0.000017\n",
      "2022-04-06 11:39:17,233 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:39:17,234 EPOCH 7 done: loss 0.0046 - lr 0.0000167\n",
      "2022-04-06 11:40:09,813 DEV : loss 0.2860768437385559 - f1-score (micro avg)  0.8454\n",
      "2022-04-06 11:40:10,040 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 11:40:10,043 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:41:47,038 epoch 8 - iter 97/970 - loss 0.00020420 - samples/sec: 6.11 - lr: 0.000016\n",
      "2022-04-06 11:42:49,144 epoch 8 - iter 194/970 - loss 0.00501603 - samples/sec: 6.25 - lr: 0.000016\n",
      "2022-04-06 11:43:50,732 epoch 8 - iter 291/970 - loss 0.00342135 - samples/sec: 6.30 - lr: 0.000015\n",
      "2022-04-06 11:44:54,363 epoch 8 - iter 388/970 - loss 0.00430622 - samples/sec: 6.10 - lr: 0.000014\n",
      "2022-04-06 11:45:56,424 epoch 8 - iter 485/970 - loss 0.00351943 - samples/sec: 6.26 - lr: 0.000014\n",
      "2022-04-06 11:46:58,177 epoch 8 - iter 582/970 - loss 0.00296200 - samples/sec: 6.30 - lr: 0.000013\n",
      "2022-04-06 11:48:01,332 epoch 8 - iter 679/970 - loss 0.00398150 - samples/sec: 6.15 - lr: 0.000013\n",
      "2022-04-06 11:49:03,836 epoch 8 - iter 776/970 - loss 0.00439813 - samples/sec: 6.21 - lr: 0.000012\n",
      "2022-04-06 11:50:06,982 epoch 8 - iter 873/970 - loss 0.00404242 - samples/sec: 6.15 - lr: 0.000012\n",
      "2022-04-06 11:51:10,323 epoch 8 - iter 970/970 - loss 0.00395176 - samples/sec: 6.13 - lr: 0.000011\n",
      "2022-04-06 11:51:11,019 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:51:11,019 EPOCH 8 done: loss 0.0040 - lr 0.0000111\n",
      "2022-04-06 11:52:02,879 DEV : loss 0.2879260778427124 - f1-score (micro avg)  0.8464\n",
      "2022-04-06 11:52:03,105 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 11:52:03,107 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 11:53:40,283 epoch 9 - iter 97/970 - loss 0.00004948 - samples/sec: 6.12 - lr: 0.000011\n",
      "2022-04-06 11:54:43,889 epoch 9 - iter 194/970 - loss 0.00044256 - samples/sec: 6.10 - lr: 0.000010\n",
      "2022-04-06 11:55:47,387 epoch 9 - iter 291/970 - loss 0.00240398 - samples/sec: 6.11 - lr: 0.000009\n",
      "2022-04-06 11:56:49,537 epoch 9 - iter 388/970 - loss 0.00206683 - samples/sec: 6.25 - lr: 0.000009\n",
      "2022-04-06 11:57:52,318 epoch 9 - iter 485/970 - loss 0.00213857 - samples/sec: 6.18 - lr: 0.000008\n",
      "2022-04-06 11:58:54,259 epoch 9 - iter 582/970 - loss 0.00179400 - samples/sec: 6.27 - lr: 0.000008\n",
      "2022-04-06 11:59:58,358 epoch 9 - iter 679/970 - loss 0.00215916 - samples/sec: 6.06 - lr: 0.000007\n",
      "2022-04-06 12:01:00,452 epoch 9 - iter 776/970 - loss 0.00189457 - samples/sec: 6.25 - lr: 0.000007\n",
      "2022-04-06 12:02:03,015 epoch 9 - iter 873/970 - loss 0.00184818 - samples/sec: 6.21 - lr: 0.000006\n",
      "2022-04-06 12:03:04,818 epoch 9 - iter 970/970 - loss 0.00224918 - samples/sec: 6.28 - lr: 0.000006\n",
      "2022-04-06 12:03:05,527 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 12:03:05,528 EPOCH 9 done: loss 0.0022 - lr 0.0000056\n",
      "2022-04-06 12:03:56,107 DEV : loss 0.27916282415390015 - f1-score (micro avg)  0.8495\n",
      "2022-04-06 12:03:56,340 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 12:03:56,341 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 12:05:33,105 epoch 10 - iter 97/970 - loss 0.00003591 - samples/sec: 6.13 - lr: 0.000005\n",
      "2022-04-06 12:06:35,267 epoch 10 - iter 194/970 - loss 0.00032536 - samples/sec: 6.25 - lr: 0.000004\n",
      "2022-04-06 12:07:38,562 epoch 10 - iter 291/970 - loss 0.00100171 - samples/sec: 6.13 - lr: 0.000004\n",
      "2022-04-06 12:08:40,883 epoch 10 - iter 388/970 - loss 0.00094513 - samples/sec: 6.23 - lr: 0.000003\n",
      "2022-04-06 12:09:43,200 epoch 10 - iter 485/970 - loss 0.00168213 - samples/sec: 6.23 - lr: 0.000003\n",
      "2022-04-06 12:10:45,344 epoch 10 - iter 582/970 - loss 0.00171904 - samples/sec: 6.25 - lr: 0.000002\n",
      "2022-04-06 12:11:47,074 epoch 10 - iter 679/970 - loss 0.00154576 - samples/sec: 6.29 - lr: 0.000002\n",
      "2022-04-06 12:12:52,002 epoch 10 - iter 776/970 - loss 0.00144863 - samples/sec: 5.98 - lr: 0.000001\n",
      "2022-04-06 12:13:59,658 epoch 10 - iter 873/970 - loss 0.00139722 - samples/sec: 5.74 - lr: 0.000001\n",
      "2022-04-06 12:15:03,299 epoch 10 - iter 970/970 - loss 0.00135418 - samples/sec: 6.10 - lr: 0.000000\n",
      "2022-04-06 12:15:03,988 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 12:15:03,989 EPOCH 10 done: loss 0.0014 - lr 0.0000000\n",
      "2022-04-06 12:15:55,412 DEV : loss 0.2769726514816284 - f1-score (micro avg)  0.8485\n",
      "2022-04-06 12:15:55,638 BAD EPOCHS (no improvement): 4\n",
      "2022-04-06 12:16:02,529 ----------------------------------------------------------------------------------------------------\n",
      "2022-04-06 12:16:02,530 Testing using last state of model ...\n",
      "2022-04-06 12:16:39,574 0.2804\t0.2804\t0.2804\t0.2804\n",
      "2022-04-06 12:16:39,575 \n",
      "Results:\n",
      "- F-score (micro) 0.2804\n",
      "- F-score (macro) 0.2984\n",
      "- Accuracy 0.2804\n",
      "\n",
      "By class:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive     0.9565    0.1667    0.2839       132\n",
      "     neutral     0.0340    1.0000    0.0658         5\n",
      "    negative     0.7500    0.4286    0.5455        77\n",
      "\n",
      "   micro avg     0.2804    0.2804    0.2804       214\n",
      "   macro avg     0.5802    0.5317    0.2984       214\n",
      "weighted avg     0.8607    0.2804    0.3729       214\n",
      " samples avg     0.2804    0.2804    0.2804       214\n",
      "\n",
      "2022-04-06 12:16:39,575 ----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_score': 0.2803738317757009,\n",
       " 'dev_score_history': [0.8443298969072165,\n",
       "  0.8422680412371134,\n",
       "  0.845360824742268,\n",
       "  0.8463917525773197,\n",
       "  0.8597938144329897,\n",
       "  0.8515463917525773,\n",
       "  0.845360824742268,\n",
       "  0.8463917525773197,\n",
       "  0.8494845360824742,\n",
       "  0.8484536082474226],\n",
       " 'train_loss_history': [0.16780760888677948,\n",
       "  0.09549411863138393,\n",
       "  0.044806086490443646,\n",
       "  0.02106459706896297,\n",
       "  0.009769946744526857,\n",
       "  0.008302666715282904,\n",
       "  0.004631392323485716,\n",
       "  0.003951762071409444,\n",
       "  0.0022491757744879876,\n",
       "  0.0013541752556684378],\n",
       " 'dev_loss_history': [tensor(0.1173),\n",
       "  tensor(0.1339),\n",
       "  tensor(0.1724),\n",
       "  tensor(0.1953),\n",
       "  tensor(0.2281),\n",
       "  tensor(0.2275),\n",
       "  tensor(0.2861),\n",
       "  tensor(0.2879),\n",
       "  tensor(0.2792),\n",
       "  tensor(0.2770)]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.datasets import TREC_6\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "\n",
    "# 1. get the corpus\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = '../../Data/Training/finacial_data_train_4897'\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {1: \"text\", 0: \"label\"}\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         test_file='test.csv',\n",
    "                                         train_file='train.csv', \n",
    "                                         dev_file='val.csv',              \n",
    "                                         label_type='polarity',\n",
    "                                         delimiter=','\n",
    ") \n",
    "print(corpus)\n",
    "\n",
    "# 2. what label do we want to predict?\n",
    "# label_type = 'polarity'\n",
    "\n",
    "# 3. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary(label_type='polarity')\n",
    "\n",
    "# 4. initialize transformer document embeddings (many models are available)\n",
    "document_embeddings = TransformerDocumentEmbeddings('distilbert-base-uncased', fine_tune=True)\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict, label_type='polarity')\n",
    "\n",
    "# 6. initialize trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. run training with fine-tuning\n",
    "trainer.fine_tune('resources/taggers/financial_models',\n",
    "                  learning_rate=5.0e-5,\n",
    "                  mini_batch_size=4,\n",
    "                  max_epochs=10,\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-06 06:27:25,152 loading file resources/taggers/question-classification-with-transformer/final-model.pt\n",
      "[HUM (1.0)]\n"
     ]
    }
   ],
   "source": [
    "classifier = TextClassifier.load('resources/taggers/financial_models/final-model.pt')\n",
    "\n",
    "# create example sentence\n",
    "sentence = Sentence('Who built the Eiffel Tower ?')\n",
    "\n",
    "# predict class and print\n",
    "classifier.predict(sentence)\n",
    "\n",
    "print(sentence.labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aadae092c2ccce02241903eb00013032cfcda01a6700d12e3ae101f1192645be"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
